│                                     Step 1: Image Preprocessing                                      │
│                                                                                                      │
│ Location: xraygpt/processors/blip_processors.py                                                      │
│                                                                                                      │
│                                                                                                      │
│  # Image preprocessing transforms                                                                    │
│  class Blip2ImageEvalProcessor(BlipImageBaseProcessor):                                              │
│      def __init__(self, image_size=224, mean=None, std=None):                                        │
│          super().__init__(mean=mean, std=std)                                                        │
│                                                                                                      │
│          # Standard ImageNet normalization values                                                    │
│          if mean is None:                                                                            │
│              mean = (0.48145466, 0.4578275, 0.40821073)                                              │
│          if std is None:                                                                             │
│              std = (0.26862954, 0.26130258, 0.27577711)                                              │
│                                                                                                      │
│          self.transform = transforms.Compose([                                                       │
│              transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),   │
│              transforms.ToTensor(),  # Convert PIL to tensor [0,1]                                   │
│              self.normalize,         # Normalize with ImageNet stats                                 │
│          ])                                                                                          │
│                                                                                                      │
│      def __call__(self, item):                                                                       │
│          return self.transform(item)  # Returns: [3, 224, 224] tensor                                │
│                                                                                                      │
│                                                                                                      │
│ Process:                                                                                             │
│                                                                                                      │
│  1 Resize: X-ray image → 224×224 pixels (bicubic interpolation)                                      │
│  2 ToTensor: PIL Image → PyTorch tensor [3, 224, 224] with values [0,1]                              │
│  3 Normalize: Apply ImageNet mean/std normalization                                                  │
│                                                                                                      │
│                            Step 2: Image to Patches (Vision Transformer)                             │
│                                                                                                      │
│ Location: xraygpt/models/eva_vit.py                                                                  │
│                                                                                                      │
│                                                                                                      │
│  class PatchEmbed(nn.Module):                                                                        │
│      def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):                     │
│          super().__init__()                                                                          │
│          img_size = to_2tuple(img_size)          # (224, 224)                                        │
│          patch_size = to_2tuple(patch_size)      # (16, 16)                                          │
│          num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])  # 196 patch  │
│                                                                                                      │
│          # Convolutional layer that acts as patch embedding                                          │
│          self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)       │
│                                                                                                      │
│      def forward(self, x, **kwargs):                                                                 │
│          B, C, H, W = x.shape  # [batch, 3, 224, 224]                                                │
│          # Conv2d with kernel=16, stride=16 creates non-overlapping patches                          │
│          x = self.proj(x).flatten(2).transpose(1, 2)  # [batch, 196, 768]                            │
│          return x                                                                                    │
│                                                                                                      │
│                                                                                                      │
│ Process:                                                                                             │
│                                                                                                      │
│  1 Input: [batch, 3, 224, 224] normalized image tensor                                               │
│  2 Patch Extraction: 16×16 patches → 196 patches total (224/16 = 14, 14×14 = 196)                    │
│  3 Linear Projection: Each patch → 768-dimensional embedding                                         │
│  4 Output: [batch, 196, 768] patch embeddings                                                        │
│                                                                                                      │
│                                Step 3: Vision Transformer Processing                                 │
│                                                                                                      │
│ Location: xraygpt/models/eva_vit.py                                                                  │
│                                                                                                      │
│                                                                                                      │
│  class VisionTransformer(nn.Module):                                                                 │
│      def forward_features(self, x):                                                                  │
│          x = self.patch_embed(x)  # [batch, 196, 768]                                                │
│          batch_size, seq_len, _ = x.size()                                                           │
│                                                                                                      │
│          # Add CLS token                                                                             │
│          cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # [batch, 1, 768]                   │
│          x = torch.cat((cls_tokens, x), dim=1)  # [batch, 197, 768]                                  │
│                                                                                                      │
│          # Add positional embeddings                                                                 │
│          if self.pos_embed is not None:                                                              │
│              x = x + self.pos_embed  # [batch, 197, 768]                                             │
│          x = self.pos_drop(x)                                                                        │
│                                                                                                      │
│          # Pass through transformer blocks                                                           │
│          rel_pos_bias = self.rel_pos_bias() if self.rel_pos_bias is not None else None               │
│          for blk in self.blocks:                                                                     │
│              x = blk(x, rel_pos_bias)  # Self-attention + MLP                                        │
│                                                                                                      │
│          return x  # [batch, 197, 1408] (EVA-CLIP-G has 1408 dim)                                    │
│                                                                                                      │
│                                                                                                      │
│ Process:                                                                                             │
│                                                                                                      │
│  1 Patch Embedding: [batch, 196, 768] → [batch, 196, 1408] (EVA-CLIP-G)                              │
│  2 CLS Token: Add learnable [CLS] token → [batch, 197, 1408]                                         │
│  3 Positional Encoding: Add learned position embeddings                                              │
│  4 Transformer Blocks: 39 layers of self-attention + MLP                                             │
│  5 Output: [batch, 197, 1408] contextualized patch representations 

 Step 4: Q-Former Cross-Attention                                   │
│                                                                                                      │
│ Location: xraygpt/models/mini_gpt4.py                                                                │
│                                                                                                      │
│                                                                                                      │
│  def encode_img(self, image):                                                                        │
│      device = image.device                                                                           │
│                                                                                                      │
│      # Step 1: Get vision features from EVA-CLIP                                                     │
│      with self.maybe_autocast():                                                                     │
│          image_embeds = self.ln_vision(self.visual_encoder(image)).to(device)  # [batch, 197, 1408]  │
│          image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(device)  # [batch,   │
│  197]                                                                                                │
│                                                                                                      │
│          # Step 2: Q-Former cross-attention                                                          │
│          query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)  # [batch, 32, 768]  │
│          query_output = self.Qformer.bert(                                                           │
│              query_embeds=query_tokens,           # Learnable queries [batch, 32, 768]               │
│              encoder_hidden_states=image_embeds,  # Vision features [batch, 197, 1408]               │
│              encoder_attention_mask=image_atts,   # Attention mask [batch, 197]                      │
│              return_dict=True,                                                                       │
│          )                                                                                           │
│                                                                                                      │
│          # Step 3: Project to LLM space                                                              │
│          inputs_llama = self.llama_proj(query_output.last_hidden_state)  # [batch, 32, 4096]         │
│          atts_llama = torch.ones(inputs_llama.size()[:-1], dtype=torch.long).to(image.device)        │
│                                                                                                      │
│      return inputs_llama, atts_llama  # [batch, 32, 4096], [batch, 32]                               │
│                                                                                                      │
│                                                                                                      │
│ Process:                                                                                             │
│                                                                                                      │
│  1 Layer Norm: Apply layer normalization to vision features                                          │
│  2 Query Tokens: 32 learnable query embeddings [batch, 32, 768]                                      │
│  3 Cross-Attention: Queries attend to all 197 vision patches                                         │
│  4 Linear Projection: [batch, 32, 768] → [batch, 32, 4096] (LLaMA hidden size)                       │
│  5 Output: 32 vision-language aligned tokens ready for LLM                                           │
│                                                            

2. TEXT PROCESSING PIPELINE                                      │
│                                                                                                      │
│                                      Step 1: Text Preprocessing                                      │
│                                                                                                      │
│ Location: xraygpt/processors/blip_processors.py                                                      │
│                                                                                                      │
│                                                                                                      │
│  class BlipCaptionProcessor(BaseProcessor):                                                          │
│      def pre_caption(self, caption):                                                                 │
│          # Clean and normalize text                                                                  │
│          caption = re.sub(r"([.!\"()*#:;~])", " ", caption.lower())  # Remove punctuation            │
│          caption = re.sub(r"\s{2,}", " ", caption)                   # Remove extra spaces           │
│          caption = caption.rstrip("\n").strip(" ")                   # Remove newlines/spaces        │
│                                                                                                      │
│          # Truncate if too long                                                                      │
│          caption_words = caption.split(" ")                                                          │
│          if len(caption_words) > self.max_words:                                                     │
│              caption = " ".join(caption_words[:self.max_words])                                      │
│                                                                                                      │
│          return caption                                                                              │
│                                                                                                      │
│                                                                                                      │
│                                 Step 2: Prompt Template Application                                  │
│                                                                                                      │
│ Location: xraygpt/models/mini_gpt4.py                                                                │
│                                                                                                      │
│                                                                                                      │
│  def prompt_wrap(self, img_embeds, atts_img, prompt):                                                │
│      if prompt:                                                                                      │
│          batch_size = img_embeds.shape[0]                                                            │
│          # Split prompt at image placeholder                                                         │
│          p_before, p_after = prompt.split('<ImageHere>')  # "###Patient: " + " ###Doctor: "          │
│                                                                                                      │
│          # Tokenize text parts                                                                       │
│          p_before_tokens = self.llama_tokenizer(                                                     │
│              p_before, return_tensors="pt", add_special_tokens=False).to(img_embeds.device)          │
│          p_after_tokens = self.llama_tokenizer(                                                      │
│              p_after, return_tensors="pt", add_special_tokens=False).to(img_embeds.device)           │
│                                                                                                      │
│          # Convert to embeddings                                                                     │
│          p_before_embeds =                                                                           │
│  self.llama_model.model.embed_tokens(p_before_tokens.input_ids).expand(batch_size, -1, -1)           │
│          p_after_embeds =                                                                            │
│  self.llama_model.model.embed_tokens(p_after_tokens.input_ids).expand(batch_size, -1, -1)            │
│                                                                                                      │
│          # Concatenate: [text_before] + [image_tokens] + [text_after]                                │
│          wrapped_img_embeds = torch.cat([p_before_embeds, img_embeds, p_after_embeds], dim=1)        │
│          wrapped_atts_img = atts_img[:, :1].expand(-1, wrapped_img_embeds.shape[1])                  │
│                                                                                                      │
│          return wrapped_img_embeds, wrapped_atts_img                                                 │
│      else:                                                                                           │
│          return img_embeds, atts_img                                                                 │
│                                                                                                      │
│                                                                                                      │
│                                      Step 3: Text Tokenization                                       │
│                                                                                                      │
│ Location: xraygpt/models/mini_gpt4.py                                                                │
│                                                                                                      │
│                                                                                                      │
│  # During training/inference                                                                         │
│  text = [t + self.end_sym for t in samples["caption"]]  # Add "###" end symbol                       │
│                                                                                                      │
│  to_regress_tokens = self.llama_tokenizer(                                                           │
│      text,                                                                                           │
│      return_tensors="pt",                                                                            │
│      padding="longest",      # Pad to longest sequence in batch                                      │
│      truncation=True,        # Truncate if too long                                                  │
│      max_length=self.max_txt_len,  # Max 160 tokens                                                  │
│      add_special_tokens=False                                                                        │
│  ).to(image.device)                                                                                  │
│                                                                                                      │
│  # Convert token IDs to embeddings                                                                   │
│  to_regress_embeds = self.llama_model.model.embed_tokens(to_regress_tokens.input_ids)                │
│                                                                                                      │
│                                                                                                      │
│ Process:                                                                                             │
│                                                                                                      │
│  1 Template: "###Patient: {user_query} ###Doctor: "                                                  │
│  2 Tokenization: Text → token IDs using LLaMA tokenizer                                              │
│  3 Embedding: Token IDs → embeddings [batch, seq_len, 4096]                                          │
│                                                                                                      │
│ ──────────────────────────────────────────────────────────────────────────────────────────────────── │
│                                                                                                      │
│                                         3. MULTIMODAL FUSION                                         │
│                                                                                                      │
│                                Step 1: Conversation Context Building                                 │
│                                                                                                      │
│ Location: xraygpt/conversation/conversation.py                                                       │
│                                                                                                      │
│                                                                                                      │
│  def get_context_emb(self, conv, img_list):                                                          │
│      prompt = conv.get_prompt()  # Get full conversation prompt                                      │
│      prompt_segs = prompt.split('<ImageHere>')  # Split at image placeholders                        │
│                                                                                                      │
│      # Tokenize text segments                                                                        │
│      seg_tokens = [                                                                                  │
│          self.model.llama_tokenizer(                                                                 │
│              seg, return_tensors="pt", add_special_tokens=i == 0).to(self.device).input_ids          │
│          for i, seg in enumerate(prompt_segs)                                                        │
│      ]                                                                                               │
│                                                                                                      │
│      # Convert to embeddings                                                                         │
│      seg_embs = [self.model.llama_model.model.embed_tokens(seg_t) for seg_t in seg_tokens]           │
│                                                                                                      │
│      # Interleave text and image embeddings                                                          │
│      mixed_embs = [emb for pair in zip(seg_embs[:-1], img_list) for emb in pair] + [seg_embs[-1]]    │
│      mixed_embs = torch.cat(mixed_embs, dim=1)  # Final multimodal sequence                          │
│                                                                                                      │
│      return mixed_embs                                                                               │
│                                                                                                      │
│                                                                                                      │
│                              Step 2: Final Input Sequence Construction                               │
│                                                                                                      │
│ Location: xraygpt/models/mini_gpt4.py                                                                │
│                                                                                                      │
│                                                                                                      │
│  def forward(self, samples):                                                                         │
│      # Get image embeddings                                                                          │
│      img_embeds, atts_img = self.encode_img(image)  # [batch, 32, 4096]                              │
│                                                                                                      │
│      # Apply prompt template                                                                         │
│      if self.prompt_list:                                                                            │
│          prompt = random.choice(self.prompt_list)  # Random medical prompt                           │
│          img_embeds, atts_img = self.prompt_wrap(img_embeds, atts_img, prompt)                       │
│                                                                                                      │
│      # Process target text                                                                           │
│      text = [t + self.end_sym for t in samples["caption"]]                                           │
│      to_regress_tokens = self.llama_tokenizer(text, ...)                                             │
│      to_regress_embeds = self.llama_model.model.embed_tokens(to_regress_tokens.input_ids)            │
│                                                                                                      │
│      # Build final sequence: [BOS] + [prompt_before] + [image_tokens] + [prompt_after] +             │
│  [target_text]                                                                                       │
│      batch_size = img_embeds.shape[0]                                                                │
│      bos = torch.ones([batch_size, 1], dtype=torch.long, device=device) *                            │
│  self.llama_tokenizer.bos_token_id                                                                   │
│      bos_embeds = self.llama_model.model.embed_tokens(bos)                                           │
│                                                                                                      │
│      inputs_embeds = torch.cat([bos_embeds, img_embeds, to_regress_embeds], dim=1)                   │
│      attention_mask = torch.cat([atts_bos, atts_img, to_regress_tokens.attention_mask], dim=1)       │
│                                                                                                      │
│      return inputs_embeds, attention_mask                                                            │
│                                                                                                      │
│                                                                                                      │
│ ──────────────────────────────────────────────────────────────────────────────────────────────────── │
│                                                                                                      │
│                                      4. LLM GENERATION PROCESS                                       │
│                                                                                                      │
│                                  Step 1: Forward Pass Through LLaMA                                  │
│                                                                                                      │
│ Location: xraygpt/models/mini_gpt4.py                                                                │
│                                                                                                      │
│                                                                                                      │
│  def forward(self, samples):                                                                         │
│      # ... (multimodal fusion above)                                                                 │
│                                                                                                      │
│      with self.maybe_autocast():                                                                     │
│          outputs = self.llama_model(                                                                 │
│              inputs_embeds=inputs_embeds,    # [batch, total_seq_len, 4096]                          │
│              attention_mask=attention_mask,  # [batch, total_seq_len]                                │
│              return_dict=True,                                                                       │
│              labels=targets,                 # For training loss                                     │
│          )                                                                                           │
│      loss = outputs.loss                                                                             │
│      return {"loss": loss}                                                                           │
│                                                                                                      │
│                                                                                                      │
│                                     Step 2: Inference Generation                                     │
│                                                                                                      │
│ Location: xraygpt/conversation/conversation.py                                                       │
│                                                                                                      │
│                                                                                                      │
│  def answer(self, conv, img_list, max_new_tokens=300, num_beams=1, temperature=1.0, ...):            │
│      conv.append_message(conv.roles[1], None)  # Add doctor role                                     │
│      embs = self.get_context_emb(conv, img_list)  # Get multimodal context                           │
│                                                                                                      │
│      # Generate response                                                                             │
│      outputs = self.model.llama_model.generate(                                                      │
│          inputs_embeds=embs,                                                                         │
│          max_new_tokens=max_new_tokens,                                                              │
│          stopping_criteria=self.stopping_criteria,  # Stop at "###"                                  │
│          num_beams=num_beams,                                                                        │
│          do_sample=True,                                                                             │
│          temperature=temperature,                                                                    │
│          top_p=0.9,                                                                                  │
│      )                                                                                               │
│                                                                                                      │
│      # Decode and clean output                                                                       │
│      output_token = outputs[0]                                                                       │
│      output_text = self.model.llama_tokenizer.decode(output_token, add_special_tokens=False)         │
│      output_text = output_text.split('###')[0]  # Remove stop token                                  │
│      output_text = output_text.split('Doctor:')[-1].strip()  # Extract doctor response               │
│                                                                                                      │
│      return output_text                                                                              │
│                                                                                                      │
│                                                                                                      │
│ ──────────────────────────────────────────────────────────────────────────────────────────────────── │
│                                                                                                      │
│                                       5. COMPLETE EXAMPLE FLOW                                       │
│                                                                                                      │
│                                                                                                      │
│  # Input Example                                                                                     │
│  image = PIL.Image.open("chest_xray.jpg")  # X-ray image                                             │
│  user_query = "What do you see in this chest X-ray?"                                                 │
│                                                                                                      │
│  # Step 1: Image Processing                                                                          │
│  processed_image = vis_processor(image)  # [3, 224, 224]                                             │
│  image_patches = patch_embed(processed_image)  # [196, 768] patches                                  │
│  vision_features = vision_transformer(image_patches)  # [197, 1408]                                  │
│  image_tokens = qformer_cross_attention(vision_features)  # [32, 4096]                               │
│                                                                                                      │
│  # Step 2: Text Processing                                                                           │
│  prompt = "###Patient: What do you see in this chest X-ray? <ImageHere> ###Doctor: "                 │
│  prompt_tokens = llama_tokenizer(prompt)  # Token IDs                                                │
│  prompt_embeds = embed_tokens(prompt_tokens)  # [seq_len, 4096]                                      │
│                                                                                                      │
│  # Step 3: Multimodal Fusion                                                                         │
│  multimodal_sequence = concat([                                                                      │
│      bos_embed,      # [1, 4096]                                                                     │
│      prompt_before,  # [n, 4096] "###Patient: What do you see in this chest X-ray?"                  │
│      image_tokens,   # [32, 4096] Visual information                                                 │
│      prompt_after    # [m, 4096] "###Doctor: "                                                       │
│  ])  # Final: [1+n+32+m, 4096]                                                                       │
│                                                                                                      │
│  # Step 4: LLM Generation                                                                            │
│  response = llama_model.generate(                                                                    │
│      inputs_embeds=multimodal_sequence,                                                              │
│      max_new_tokens=300,                                                                             │
│      temperature=1.0                                                                                 │
│  )                                                                                                   │
│                                                                                                      │
│  # Output: "The chest X-ray shows clear lung fields with no evidence of consolidation,               │
│  #          pleural effusion, or pneumothorax. The cardiac silhouette appears normal..."             │
│                                                                                                      │
│                                                                                                      │
│ This comprehensive pipeline ensures that both visual and textual information are properly encoded,   │
│ aligned, and fused before being processed by the language model to generate medically relevant       │
│ responses.                                                                                           │
│                                                                                                      │
│ What specific aspect of this multimodal processing would you like me to explore further? I can dive  │
│ deeper into:                                                                                         │
│                                                                                                      │
│  1 The Q-Former cross-attention mechanism details                                                    │
│  2 The vision transformer patch processing specifics                                                 │
│  3 The prompt engineering and conversation management                                                │
│  4 The training loss computation and optimization                                                    │
│  5 The generation strategies and stopping criteria   
