# Custom Domain Configuration Template
# Copy and modify this template for your specific domain

domain:
  name: "YOUR_DOMAIN_NAME"  # e.g., "satellite", "legal", "financial"
  description: "Description of your domain"

validation:
  domain: "YOUR_DOMAIN_NAME"
  domain_name: "YOUR_DOMAIN_NAME"
  min_image_size: [224, 224]  # Adjust for your image requirements
  max_image_size: [1024, 1024]
  min_text_length: 5
  max_text_length: 1000
  forbidden_text_patterns: []  # Add patterns to filter out
  required_channels: [1, 3]  # Image channel requirements
  require_domain_content: false  # Set to true to enforce domain content
  convert_to_grayscale: false
  apply_histogram_equalization: false
  
  # Add keywords specific to your domain
  domain_keywords: 
    - "keyword1"
    - "keyword2"
    - "keyword3"
  
  # Add domain-specific text standardizations
  domain_replacements:
    "old_term": "new_term"
    "abbreviation": "full_form"

tokenization:
  text:
    type: "text"
    tokenizer_type: "bert"  # Choose: bert, llama, gpt2, t5, auto
    model_name: "bert-base-uncased"  # Or domain-specific model
    max_length: 512
    padding: "max_length"
    truncation: true
    add_special_tokens: true
    special_tokens: {}  # Add domain-specific tokens
  
  image:
    type: "vision"
    method: "patch"
    patch_size: 16  # Adjust based on your image characteristics
    image_size: 224
    vocab_size: 8192

encoding:
  device: "cuda"  # or "cpu"
  
  text:
    type: "bert"  # Choose: bert, roberta, clip_text, medical_bert
    model_name: "bert-base-uncased"  # Or domain-specific encoder
    hidden_size: 768
    device: "cuda"
    pooling_strategy: "cls"  # Choose: cls, mean, max
    freeze_encoder: false
    encoder_type: "bert"
    
  image:
    type: "vision_transformer"  # Choose: vision_transformer, clip_vision, resnet
    model_name: "google/vit-base-patch16-224"  # Or domain-specific encoder
    hidden_size: 768
    device: "cuda"
    freeze_encoder: false
    encoder_type: "vision_transformer"
    image_size: 224
    patch_size: 16

attention:
  self_attention:
    enabled: true
    num_attention_heads: 12
    hidden_size: 768
    attention_dropout: 0.1
    layer_norm_eps: 1e-12
    
  cross_attention:
    enabled: true
    num_attention_heads: 12
    hidden_size: 768
    attention_dropout: 0.1
    cross_attention_freq: 2  # Adjust based on your needs
    save_attention: false  # Set to true for analysis
    
  qformer:
    enabled: false  # Enable for complex multimodal reasoning
    num_query_tokens: 32
    num_layers: 6
    cross_attention_freq: 2

decoding:
  type: "llm"
  model_name: "microsoft/DialoGPT-medium"  # Choose appropriate LLM
  max_new_tokens: 128  # Adjust based on expected output length
  temperature: 0.7  # Control randomness (0.0 = deterministic, 1.0 = random)
  top_p: 0.9  # Nucleus sampling parameter
  top_k: 50  # Top-k sampling parameter
  do_sample: true  # Enable sampling for diverse outputs
  num_beams: 1  # Beam search (1 = no beam search)
  repetition_penalty: 1.0  # Penalize repetition
  length_penalty: 1.0  # Control output length preference
  early_stopping: true
  pad_token_id: 50256  # Adjust based on your tokenizer
  eos_token_id: 50256

model:
  name: "CustomMultiModalModel"
  device: "cuda"
  dtype: "float16"  # Choose: float16, float32, bfloat16
  gradient_checkpointing: false
  training_mode: false

# Optional: Training configuration
training:
  learning_rate: 1e-4
  weight_decay: 0.01
  warmup_steps: 1000
  max_steps: 10000
  gradient_accumulation_steps: 1
  max_grad_norm: 1.0

# Optional: Evaluation configuration
evaluation:
  batch_size: 8
  metrics: ["bleu", "rouge"]
  save_predictions: true