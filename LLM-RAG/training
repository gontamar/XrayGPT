ðŸ” Detailed Multimodal Processing Flow                                â”‚
â”‚                                                                                                      â”‚
â”‚                                     1. IMAGE PROCESSING PIPELINE                                     â”‚
â”‚                                                                                                      â”‚
â”‚                                     Step 1: Image Preprocessing                                      â”‚
â”‚                                                                                                      â”‚
â”‚                                                                                                      â”‚
â”‚  # File: xraygpt/processors/blip_processors.py                                                       â”‚
â”‚  class Blip2ImageTrainProcessor:                                                                     â”‚
â”‚      def __init__(self, image_size=224, min_scale=0.5, max_scale=1.0):                               â”‚
â”‚          self.transform = transforms.Compose([                                                       â”‚
â”‚              transforms.RandomResizedCrop(                                                           â”‚
â”‚                  image_size,                                                                         â”‚
â”‚                  scale=(min_scale, max_scale),           # Random crop 50-100%                       â”‚
â”‚                  interpolation=InterpolationMode.BICUBIC,                                            â”‚
â”‚              ),                                                                                      â”‚
â”‚              transforms.ToTensor(),                      # Convert to [0,1] tensor                   â”‚
â”‚              transforms.Normalize(                                                                   â”‚
â”‚                  mean=(0.48145466, 0.4578275, 0.40821073),  # ImageNet normalization                 â”‚
â”‚                  std=(0.26862954, 0.26130258, 0.27577711)                                            â”‚
â”‚              ),                                                                                      â”‚
â”‚          ])                                                                                          â”‚
â”‚                                                                                                      â”‚
â”‚                                                                                                      â”‚
â”‚ Output: [3, 224, 224] normalized tensor                                                              â”‚
â”‚                                                                                                      â”‚
â”‚                                       Step 2: Patch Embedding                                        â”‚
â”‚                                                                                                      â”‚
â”‚                                                                                                      â”‚
â”‚  # File: xraygpt/models/eva_vit.py                                                                   â”‚
â”‚  class PatchEmbed(nn.Module):                                                                        â”‚
â”‚      def __init__(self, img_size=224, patch_size=16, embed_dim=768):                                 â”‚
â”‚          # Convolutional patch embedding                                                             â”‚
â”‚          self.proj = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)              â”‚
â”‚                                                                                                      â”‚
â”‚      def forward(self, x):                                                                           â”‚
â”‚          B, C, H, W = x.shape                        # [batch, 3, 224, 224]                          â”‚
â”‚          x = self.proj(x).flatten(2).transpose(1, 2) # [batch, 196, 768]                             â”‚
â”‚          return x                                                                                    â”‚
â”‚                                                                                                      â”‚
â”‚                                                                                                      â”‚
â”‚ Process:                                                                                             â”‚
â”‚                                                                                                      â”‚
â”‚  â€¢ 224Ã—224 image â†’ 16Ã—16 patches â†’ 196 patches (14Ã—14 grid)                                          â”‚
â”‚  â€¢ Each patch â†’ 1408D embedding (EVA-CLIP-G) Output: [batch, 196, 1408]                              â”‚
â”‚                                                                                                      â”‚
â”‚                                      Step 3: Vision Transformer                                      â”‚
â”‚                                                                                                      â”‚
â”‚                                                                                                      â”‚
â”‚  # File: xraygpt/models/eva_vit.py                                                                   â”‚
â”‚  class VisionTransformer:                                                                            â”‚
â”‚      def forward_features(self, x):                                                                  â”‚
â”‚          x = self.patch_embed(x)                     # [batch, 196, 1408]                            â”‚
â”‚                                                                                                      â”‚
â”‚          # Add CLS token                                                                             â”‚
â”‚          cls_tokens = self.cls_token.expand(batch_size, -1, -1)                                      â”‚
â”‚          x = torch.cat((cls_tokens, x), dim=1)       # [batch, 197, 1408]                            â”‚
â”‚                                                                                                      â”‚
â”‚          # Add positional embeddings                                                                 â”‚
â”‚          x = x + self.pos_embed                      # [batch, 197, 1408]                            â”‚
â”‚                                                                                                      â”‚
â”‚          # Process through 39 transformer blocks                                                     â”‚
â”‚          for blk in self.blocks:                                                                     â”‚
â”‚              x = blk(x, rel_pos_bias)                # Self-attention + MLP                          â”‚
â”‚                                                                                                      â”‚
â”‚          return x                                    # [batch, 197, 1408]                            â”‚
â”‚                                                                                                      â”‚
â”‚                                                                                                      â”‚
â”‚ Process:                                                                                             â”‚
â”‚                                                                                                      â”‚
â”‚  â€¢ Add CLS token: 196 â†’ 197 tokens                                                                   â”‚
â”‚  â€¢ Add positional embeddings                                                                         â”‚
â”‚  â€¢ 39 transformer layers with self-attention Output: [batch, 197, 1408] contextualized features      â”‚
â”‚                                                                                                      â”‚
â”‚                                   Step 4: Q-Former Cross-Attention                                   â”‚
â”‚                                                                                                      â”‚
â”‚                                                                                                      â”‚
â”‚  # File: xraygpt/models/mini_gpt4.py                                                                 â”‚
â”‚  def encode_img(self, image):                                                                        â”‚
â”‚      # Vision encoder                                                                                â”‚
â”‚      image_embeds = self.ln_vision(self.visual_encoder(image))  # [batch, 197, 1408]                 â”‚
â”‚                                                                                                      â”‚
â”‚      # Q-Former cross-attention                                                                      â”‚
â”‚      query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)  # [batch, 32, 768]      â”‚
â”‚      query_output = self.Qformer.bert(                                                               â”‚
â”‚          query_embeds=query_tokens,           # Learnable queries                                    â”‚
â”‚          encoder_hidden_states=image_embeds,  # Vision features as keys/values                       â”‚
â”‚          encoder_attention_mask=image_atts,   # Attention mask                                       â”‚
â”‚          return_dict=True,                                                                           â”‚
â”‚      )                                                                                               â”‚
â”‚                                                                                                      â”‚
â”‚      # Project to LLaMA dimension                                                                    â”‚
â”‚      inputs_llama = self.llama_proj(query_output.last_hidden_state)  # [batch, 32, 4096]             â”‚
â”‚      return inputs_llama, atts_llama                                                                 â”‚
â”‚                                                                                                      â”‚
â”‚                                                                                                      â”‚
â”‚ Process:                                                                                             â”‚
â”‚                                                                                                      â”‚
â”‚  â€¢ 32 learnable query tokens attend to 197 vision patches                                            â”‚
â”‚  â€¢ Cross-attention extracts relevant visual information                                              â”‚
â”‚  â€¢ Linear projection: 768D â†’ 4096D (LLaMA dimension) Output: [batch, 32, 4096] image tokens for      â”‚
â”‚    LLaMA                                                                                             â”‚
â”‚                                                                                                      â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                     2. TEXT PROCESSING PIPELINE                                      â”‚
â”‚                                                                                                      â”‚
â”‚                                      Step 1: Text Preprocessing                                      â”‚
â”‚                                                                                                      â”‚
â”‚                                                                                                      â”‚
â”‚  # File: xraygpt/processors/blip_processors.py                                                       â”‚
â”‚  class BlipCaptionProcessor:                                                                         â”‚
â”‚      def pre_caption(self, caption):                                                                 â”‚
â”‚          # Clean punctuation and normalize                                                           â”‚
â”‚          caption = re.sub(r"([.!\"()*#:;~])", " ", caption.lower())                                  â”‚
â”‚          caption = re.sub(r"\s{2,}", " ", caption)                                                   â”‚
â”‚          caption = caption.rstrip("\n").strip(" ")                                                   â”‚
â”‚                                                                                                      â”‚
â”‚          # Truncate to max words                                                                     â”‚
â”‚          caption_words = caption.split(" ")                                                          â”‚
â”‚          if len(caption_words) > self.max_words:                                                     â”‚
â”‚              caption = " ".join(caption_words[:self.max_words])                                      â”‚
â”‚                                                                                                      â”‚
â”‚          return caption                                                                              â”‚
â”‚                                                                                                      â”‚
â”‚                                                                                                      â”‚
â”‚                                 Step 2: Prompt Template Application                                  â”‚
â”‚                                                                                                      â”‚
â”‚                                                                                                      â”‚
â”‚  # File: xraygpt/models/mini_gpt4.py                                                                 â”‚
â”‚  def prompt_wrap(self, img_embeds, atts_img, prompt):                                                â”‚
â”‚      # Split prompt at image placeholder                                                             â”‚
â”‚      p_before, p_after = prompt.split('<ImageHere>')                                                 â”‚
â”‚      # Example: "###Patient: " + "<ImageHere>" + " Describe this X-ray. ###Doctor: "                 â”‚
â”‚                                                                                                      â”‚
â”‚      # Tokenize and embed text segments                                                              â”‚
â”‚      p_before_embeds = self.llama_model.model.embed_tokens(p_before_tokens.input_ids)                â”‚
â”‚      p_after_embeds = self.llama_model.model.embed_tokens(p_after_tokens.input_ids)                  â”‚
â”‚                                                                                                      â”‚
â”‚      # Concatenate: [text_before] + [image_tokens] + [text_after]                                    â”‚
â”‚      wrapped_img_embeds = torch.cat([p_before_embeds, img_embeds, p_after_embeds], dim=1)            â”‚
â”‚      return wrapped_img_embeds, wrapped_atts_img                                                     â”‚
â”‚                                                                                                      â”‚
â”‚                                                                                                      â”‚
â”‚                                   Step 3: Target Text Tokenization                                   â”‚
â”‚                                                                                                      â”‚
â”‚                                                                                                      â”‚
â”‚  # File: xraygpt/models/mini_gpt4.py                                                                 â”‚
â”‚  def forward(self, samples):                                                                         â”‚
â”‚      # Process target text                                                                           â”‚
â”‚      text = [t + self.end_sym for t in samples["caption"]]  # Add "###" end symbol                   â”‚
â”‚      to_regress_tokens = self.llama_tokenizer(                                                       â”‚
â”‚          text,                                                                                       â”‚
â”‚          return_tensors="pt",                                                                        â”‚
â”‚          padding="longest",                                                                          â”‚
â”‚          truncation=True,                                                                            â”‚
â”‚          max_length=self.max_txt_len,                # 160 tokens max                                â”‚
â”‚          add_special_tokens=False                                                                    â”‚
â”‚      ).to(image.device)                                                                              â”‚
â”‚                                                                                                      â”‚
â”‚      # Convert to embeddings                                                                         â”‚
â”‚      to_regress_embeds = self.llama_model.model.embed_tokens(to_regress_tokens.input_ids)            â”‚
â”‚                                                                                                      â”‚
â”‚                                                                                                      â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                         3. MULTIMODAL FUSION                                         â”‚
â”‚                                                                                                      â”‚
â”‚                                    Complete Sequence Construction                                    â”‚
â”‚                                                                                                      â”‚
â”‚                                                                                                      â”‚
â”‚  # File: xraygpt/models/mini_gpt4.py                                                                 â”‚
â”‚  def forward(self, samples):                                                                         â”‚
â”‚      # Get components                                                                                â”‚
â”‚      img_embeds, atts_img = self.encode_img(image)           # [batch, 32, 4096]                     â”‚
â”‚      img_embeds, atts_img = self.prompt_wrap(img_embeds, atts_img, prompt)                           â”‚
â”‚                                                                                                      â”‚
â”‚      # BOS token                                                                                     â”‚
â”‚      bos = torch.ones([batch_size, 1]) * self.llama_tokenizer.bos_token_id                           â”‚
â”‚      bos_embeds = self.llama_model.model.embed_tokens(bos)   # [batch, 1, 4096]                      â”‚
â”‚                                                                                                      â”‚
â”‚      # Target text embeddings                                                                        â”‚
â”‚      to_regress_embeds = self.llama_model.model.embed_tokens(to_regress_tokens.input_ids)            â”‚
â”‚                                                                                                      â”‚
â”‚      # Final sequence: [BOS] + [prompt_before] + [image_tokens] + [prompt_after] + [target_text]     â”‚
â”‚      inputs_embeds = torch.cat([bos_embeds, img_embeds, to_regress_embeds], dim=1)                   â”‚
â”‚      attention_mask = torch.cat([atts_bos, atts_img, to_regress_tokens.attention_mask], dim=1)       â”‚
â”‚                                                                                                      â”‚
â”‚                                                                                                      â”‚
â”‚ Sequence Structure:                                                                                  â”‚
â”‚                                                                                                      â”‚
â”‚                                                                                                      â”‚
â”‚  [BOS] + [###Patient: ] + [32 image tokens] + [ Describe... ###Doctor: ] + [target medical text] +   â”‚
â”‚  [###]                                                                                               â”‚
â”‚    1   +      9 tokens   +    32 tokens     +      10 tokens           +    variable length    +  1  â”‚
â”‚                                                                                                      â”‚
â”‚                                                                                                      â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                     4. TRAINING LOSS COMPUTATION                                     â”‚
â”‚                                                                                                      â”‚
â”‚                                            Label Creation                                            â”‚
â”‚                                                                                                      â”‚
â”‚                                                                                                      â”‚
â”‚  # File: xraygpt/models/mini_gpt4.py                                                                 â”‚
â”‚  def forward(self, samples):                                                                         â”‚
â”‚      # Create labels for loss computation                                                            â”‚
â”‚      targets = to_regress_tokens.input_ids.masked_fill(                                              â”‚
â”‚          to_regress_tokens.input_ids == self.llama_tokenizer.pad_token_id, -100                      â”‚
â”‚      )                                                                                               â”‚
â”‚                                                                                                      â”‚
â”‚      # Create empty targets for image and prompt tokens (set to -100 to ignore)                      â”‚
â”‚      empty_targets = torch.ones([atts_img.shape[0], atts_img.shape[1]+1],                            â”‚
â”‚                                dtype=torch.long).fill_(-100)                                         â”‚
â”‚      targets = torch.cat([empty_targets, targets], dim=1)                                            â”‚
â”‚                                                                                                      â”‚
â”‚      # Forward through LLaMA with labels                                                             â”‚
â”‚      outputs = self.llama_model(                                                                     â”‚
â”‚          inputs_embeds=inputs_embeds,                                                                â”‚
â”‚          attention_mask=attention_mask,                                                              â”‚
â”‚          labels=targets,                                                                             â”‚
â”‚      )                                                                                               â”‚
â”‚                                                                                                      â”‚
â”‚      return {"loss": outputs.loss}                                                                   â”‚
â”‚                                                                                                      â”‚
â”‚                                                                                                      â”‚
â”‚ Loss Computation:                                                                                    â”‚
â”‚                                                                                                      â”‚
â”‚  â€¢ Ignored tokens (-100): BOS, prompt, image tokens                                                  â”‚
â”‚  â€¢ Target tokens: Only medical text response                                                         â”‚
â”‚  â€¢ Loss function: Cross-entropy on next-token prediction                                             â”‚
â”‚  â€¢ Backpropagation: Only through linear projection layer                                             â”‚
â”‚                                                                                                      â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                           5. TRAINING LOOP                                           â”‚
â”‚                                                                                                      â”‚
â”‚                                            Training Step                                             â”‚
â”‚                                                                                                      â”‚
â”‚                                                                                                      â”‚
â”‚  # File: xraygpt/tasks/base_task.py                                                                  â”‚
â”‚  def _train_inner_loop(self, epoch, iters_per_epoch, model, data_loader, optimizer, ...):            â”‚
â”‚      for i in range(iters_per_epoch):                                                                â”‚
â”‚          # Get batch                                                                                 â”‚
â”‚          samples = next(data_loader)                                                                 â”‚
â”‚          samples = prepare_sample(samples, cuda_enabled=cuda_enabled)                                â”‚
â”‚                                                                                                      â”‚
â”‚          # Forward pass                                                                              â”‚
â”‚          with torch.cuda.amp.autocast(enabled=use_amp):                                              â”‚
â”‚              loss = self.train_step(model=model, samples=samples)                                    â”‚
â”‚                                                                                                      â”‚
â”‚          # Backward pass                                                                             â”‚
â”‚          if use_amp:                                                                                 â”‚
â”‚              scaler.scale(loss).backward()                                                           â”‚
â”‚          else:                                                                                       â”‚
â”‚              loss.backward()                                                                         â”‚
â”‚                                                                                                      â”‚
â”‚          # Update weights                                                                            â”‚
â”‚          if (i + 1) % accum_grad_iters == 0:                                                         â”‚
â”‚              optimizer.step()                                                                        â”‚
â”‚              optimizer.zero_grad()                                                                   â”‚
â”‚                                                                                                      â”‚
â”‚                                                                                                      â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚                                                                                                      â”‚
â”‚                                       ðŸŽ¯ Key Training Insights                                       â”‚
â”‚                                                                                                      â”‚
â”‚                                        Parameter Efficiency:                                         â”‚
â”‚                                                                                                      â”‚
â”‚  â€¢ Total Parameters: ~8.5B (EVA-CLIP: 1.4B + Q-Former: 110M + Vicuna: 7B)                            â”‚
â”‚  â€¢ Trainable Parameters: ~98K (only linear projection: 32Ã—768â†’4096)                                  â”‚
â”‚  â€¢ Trainable Ratio: 0.001% - Extremely parameter efficient!                                          â”‚
â”‚                                                                                                      â”‚
â”‚                                          Memory Efficiency:                                          â”‚
â”‚                                                                                                      â”‚
â”‚  â€¢ Frozen Components: Vision encoder, Q-Former, LLaMA weights                                        â”‚
â”‚  â€¢ Mixed Precision: FP16 training with gradient scaling                                              â”‚
â”‚  â€¢ Gradient Accumulation: Effective larger batch sizes                                               â”‚
â”‚  â€¢ Only store gradients for 98K parameters                                                           â”‚
â”‚                                                                                                      â”‚
â”‚                                          Training Strategy:                                          â”‚
â”‚                                                                                                      â”‚
â”‚  â€¢ Stage 1: MIMIC pretraining (4 epochs, 5K iters/epoch)                                             â”‚
â”‚  â€¢ Stage 2: OpenI fine-tuning (1 epoch, 2.5K iters)                                                  â”‚
â”‚  â€¢ Focus: Vision-language alignment through linear projection                                        â”‚
â”‚  â€¢ Objective: Medical conversation generation                                                        â”‚
â”‚                                                                                                      â”‚
â”‚                                          Data Flow Summary:                                          â”‚
â”‚                                                                                                      â”‚
â”‚                                                                                                      â”‚
â”‚  X-ray Image [3,224,224]                                                                             â”‚
â”‚      â†“ Vision Processing                                                                             â”‚
â”‚  Image Tokens [32,4096]                                                                              â”‚
â”‚      â†“ Multimodal Fusion                                                                             â”‚
â”‚  Medical Text [seq_len,4096]                                                                         â”‚
â”‚      â†“ Language Generation                                                                           â”‚
â”‚  Medical Response                                                                                    â”‚
â”‚                                                                                                      â”‚
â”‚                                                                                                      â”‚
â”‚ This implementation shows exactly how XrayGPT processes multimodal inputs during training, with real â”‚
â”‚ code from the codebase and detailed explanations of each step!
