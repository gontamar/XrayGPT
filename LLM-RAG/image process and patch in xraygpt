│                                     Step 1: Image Preprocessing                                      │
│                                                                                                      │
│ Location: xraygpt/processors/blip_processors.py                                                      │
│                                                                                                      │
│                                                                                                      │
│  # Image preprocessing transforms                                                                    │
│  class Blip2ImageEvalProcessor(BlipImageBaseProcessor):                                              │
│      def __init__(self, image_size=224, mean=None, std=None):                                        │
│          super().__init__(mean=mean, std=std)                                                        │
│                                                                                                      │
│          # Standard ImageNet normalization values                                                    │
│          if mean is None:                                                                            │
│              mean = (0.48145466, 0.4578275, 0.40821073)                                              │
│          if std is None:                                                                             │
│              std = (0.26862954, 0.26130258, 0.27577711)                                              │
│                                                                                                      │
│          self.transform = transforms.Compose([                                                       │
│              transforms.Resize((image_size, image_size), interpolation=InterpolationMode.BICUBIC),   │
│              transforms.ToTensor(),  # Convert PIL to tensor [0,1]                                   │
│              self.normalize,         # Normalize with ImageNet stats                                 │
│          ])                                                                                          │
│                                                                                                      │
│      def __call__(self, item):                                                                       │
│          return self.transform(item)  # Returns: [3, 224, 224] tensor                                │
│                                                                                                      │
│                                                                                                      │
│ Process:                                                                                             │
│                                                                                                      │
│  1 Resize: X-ray image → 224×224 pixels (bicubic interpolation)                                      │
│  2 ToTensor: PIL Image → PyTorch tensor [3, 224, 224] with values [0,1]                              │
│  3 Normalize: Apply ImageNet mean/std normalization                                                  │
│                                                                                                      │
│                            Step 2: Image to Patches (Vision Transformer)                             │
│                                                                                                      │
│ Location: xraygpt/models/eva_vit.py                                                                  │
│                                                                                                      │
│                                                                                                      │
│  class PatchEmbed(nn.Module):                                                                        │
│      def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):                     │
│          super().__init__()                                                                          │
│          img_size = to_2tuple(img_size)          # (224, 224)                                        │
│          patch_size = to_2tuple(patch_size)      # (16, 16)                                          │
│          num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])  # 196 patch  │
│                                                                                                      │
│          # Convolutional layer that acts as patch embedding                                          │
│          self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)       │
│                                                                                                      │
│      def forward(self, x, **kwargs):                                                                 │
│          B, C, H, W = x.shape  # [batch, 3, 224, 224]                                                │
│          # Conv2d with kernel=16, stride=16 creates non-overlapping patches                          │
│          x = self.proj(x).flatten(2).transpose(1, 2)  # [batch, 196, 768]                            │
│          return x                                                                                    │
│                                                                                                      │
│                                                                                                      │
│ Process:                                                                                             │
│                                                                                                      │
│  1 Input: [batch, 3, 224, 224] normalized image tensor                                               │
│  2 Patch Extraction: 16×16 patches → 196 patches total (224/16 = 14, 14×14 = 196)                    │
│  3 Linear Projection: Each patch → 768-dimensional embedding                                         │
│  4 Output: [batch, 196, 768] patch embeddings                                                        │
│                                                                                                      │
│                                Step 3: Vision Transformer Processing                                 │
│                                                                                                      │
│ Location: xraygpt/models/eva_vit.py                                                                  │
│                                                                                                      │
│                                                                                                      │
│  class VisionTransformer(nn.Module):                                                                 │
│      def forward_features(self, x):                                                                  │
│          x = self.patch_embed(x)  # [batch, 196, 768]                                                │
│          batch_size, seq_len, _ = x.size()                                                           │
│                                                                                                      │
│          # Add CLS token                                                                             │
│          cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # [batch, 1, 768]                   │
│          x = torch.cat((cls_tokens, x), dim=1)  # [batch, 197, 768]                                  │
│                                                                                                      │
│          # Add positional embeddings                                                                 │
│          if self.pos_embed is not None:                                                              │
│              x = x + self.pos_embed  # [batch, 197, 768]                                             │
│          x = self.pos_drop(x)                                                                        │
│                                                                                                      │
│          # Pass through transformer blocks                                                           │
│          rel_pos_bias = self.rel_pos_bias() if self.rel_pos_bias is not None else None               │
│          for blk in self.blocks:                                                                     │
│              x = blk(x, rel_pos_bias)  # Self-attention + MLP                                        │
│                                                                                                      │
│          return x  # [batch, 197, 1408] (EVA-CLIP-G has 1408 dim)                                    │
│                                                                                                      │
│                                                                                                      │
│ Process:                                                                                             │
│                                                                                                      │
│  1 Patch Embedding: [batch, 196, 768] → [batch, 196, 1408] (EVA-CLIP-G)                              │
│  2 CLS Token: Add learnable [CLS] token → [batch, 197, 1408]                                         │
│  3 Positional Encoding: Add learned position embeddings                                              │
│  4 Transformer Blocks: 39 layers of self-attention + MLP                                             │
│  5 Output: [batch, 197, 1408] contextualized patch representations 

 Step 4: Q-Former Cross-Attention                                   │
│                                                                                                      │
│ Location: xraygpt/models/mini_gpt4.py                                                                │
│                                                                                                      │
│                                                                                                      │
│  def encode_img(self, image):                                                                        │
│      device = image.device                                                                           │
│                                                                                                      │
│      # Step 1: Get vision features from EVA-CLIP                                                     │
│      with self.maybe_autocast():                                                                     │
│          image_embeds = self.ln_vision(self.visual_encoder(image)).to(device)  # [batch, 197, 1408]  │
│          image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(device)  # [batch,   │
│  197]                                                                                                │
│                                                                                                      │
│          # Step 2: Q-Former cross-attention                                                          │
│          query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)  # [batch, 32, 768]  │
│          query_output = self.Qformer.bert(                                                           │
│              query_embeds=query_tokens,           # Learnable queries [batch, 32, 768]               │
│              encoder_hidden_states=image_embeds,  # Vision features [batch, 197, 1408]               │
│              encoder_attention_mask=image_atts,   # Attention mask [batch, 197]                      │
│              return_dict=True,                                                                       │
│          )                                                                                           │
│                                                                                                      │
│          # Step 3: Project to LLM space                                                              │
│          inputs_llama = self.llama_proj(query_output.last_hidden_state)  # [batch, 32, 4096]         │
│          atts_llama = torch.ones(inputs_llama.size()[:-1], dtype=torch.long).to(image.device)        │
│                                                                                                      │
│      return inputs_llama, atts_llama  # [batch, 32, 4096], [batch, 32]                               │
│                                                                                                      │
│                                                                                                      │
│ Process:                                                                                             │
│                                                                                                      │
│  1 Layer Norm: Apply layer normalization to vision features                                          │
│  2 Query Tokens: 32 learnable query embeddings [batch, 32, 768]                                      │
│  3 Cross-Attention: Queries attend to all 197 vision patches                                         │
│  4 Linear Projection: [batch, 32, 768] → [batch, 32, 4096] (LLaMA hidden size)                       │
│  5 Output: 32 vision-language aligned tokens ready for LLM                                           │
│                                                            
