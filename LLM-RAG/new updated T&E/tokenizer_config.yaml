# BERT Tokenizer Configuration
tokenizers:
  bert:
    model_name: "bert-base-uncased"
    special_tokens:
      bos_token: "[DEC]"
    max_length: 512
    padding: true
    truncation: true
    return_tensors: "pt"

# Text Processing Rules
input_validation:
  default:
    enabled: true
    characters:
      forbidden_characters: ["@", "#", "$", "%", "^", "&", "*"]
      forbidden_patterns: ["[\\x00-\\x1F\\x7F]"]

input_preprocessing:
  default:
    enabled: true
    whitespace:
      strip: true
      normalize_spaces: true
    characters:
      replacements:
        "\u201c": "\""
        "\u201d": "\""
      pattern_replacements:
        "\\s+": " "