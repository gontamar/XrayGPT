│ Now I'll provide a detailed line-by-line explanation of text tokenization only and how text tokens are created and   │
│ passed to the encoder in XrayGPT.                                                                                    │
│                                                                                                                      │
│                                                                                                                      │
│                                   Text Tokenization Process: Line-by-Line Analysis                                   │
│                                                                                                                      │
│                                             1. Tokenizer Initialization                                              │
│                                                                                                                      │
│                                                                                                                      │
│  # Line 107-109 in mini_gpt4.py                                                                                      │
│  self.llama_tokenizer = LlamaTokenizer.from_pretrained(llama_model, use_fast=False)                                  │
│  self.llama_tokenizer.pad_token = self.llama_tokenizer.eos_token                                                     │
│                                                                                                                      │
│                                                                                                                      │
│ Line 107:                                                                                                            │
│                                                                                                                      │
│  • LlamaTokenizer.from_pretrained(llama_model, use_fast=False): Loads the LLaMA tokenizer from the pretrained Vicuna │
│    model                                                                                                             │
│  • use_fast=False: Uses the slower but more reliable tokenizer implementation                                        │
│  • Result: Creates a tokenizer that can convert text ↔ token IDs                                                     │
│                                                                                                                      │
│ Line 109:                                                                                                            │
│                                                                                                                      │
│  • self.llama_tokenizer.pad_token = self.llama_tokenizer.eos_token: Sets padding token to be the same as             │
│    end-of-sequence token                                                                                             │
│  • Why: LLaMA doesn't have a dedicated padding token, so we use EOS token for padding                                │
│                                                                                                                      │
│                                   2. Text Processing in Training (forward method)                                    │
│                                                                                                                      │
│                                                                                                                      │
│  # Line 207-209 in mini_gpt4.py                                                                                      │
│  self.llama_tokenizer.padding_side = "right"                                                                         │
│                                                                                                                      │
│  text = [t + self.end_sym for t in samples["caption"]]                                                               │
│                                                                                                                      │
│                                                                                                                      │
│ Line 208:                                                                                                            │
│                                                                                                                      │
│  • self.llama_tokenizer.padding_side = "right": Sets tokenizer to add padding tokens on the right side of sequences  │
│  • Why: For causal language modeling, we want padding on the right so the model processes text left-to-right         │
│                                                                                                                      │
│ Line 210:                                                                                                            │
│                                                                                                                      │
│  • text = [t + self.end_sym for t in samples["caption"]]:                                                            │
│  • samples["caption"]: List of medical report texts (ground truth captions)                                          │
│  • self.end_sym: Usually "###" - marks the end of the medical report                                                 │
│  • Example: "No acute findings in chest X-ray" → "No acute findings in chest X-ray###"                               │
│                                                                                                                      │
│                                 3. Text Tokenization (Converting Text to Token IDs)                                  │
│                                                                                                                      │
│                                                                                                                      │
│  # Line 211-218 in mini_gpt4.py                                                                                      │
│  to_regress_tokens = self.llama_tokenizer(                                                                           │
│      text,                                                                                                           │
│      return_tensors="pt",                                                                                            │
│      padding="longest",                                                                                              │
│      truncation=True,                                                                                                │
│      max_length=self.max_txt_len,                                                                                    │
│      add_special_tokens=False                                                                                        │
│  ).to(image.device)                                                                                                  │
│                                                                                                                      │
│                                                                                                                      │
│ Line 211: self.llama_tokenizer(text, ...) - This is where text becomes tokens!                                       │
│                                                                                                                      │
│ Line 212: return_tensors="pt" - Returns PyTorch tensors instead of lists                                             │
│                                                                                                                      │
│ Line 213: padding="longest" - Pads all sequences in the batch to match the longest sequence                          │
│                                                                                                                      │
│  • Example: If batch has texts of length [10, 15, 8], all will be padded to length 15                                │
│                                                                                                                      │
│ Line 214: truncation=True - Cuts off text that's too long                                                            │
│                                                                                                                      │
│ Line 215: max_length=self.max_txt_len - Maximum allowed text length (usually 160 tokens)                             │
│                                                                                                                      │
│ Line 216: add_special_tokens=False - Doesn't add [CLS], [SEP] tokens (we handle special tokens manually)             │
│                                                                                                                      │
│ Line 217: .to(image.device) - Moves token tensors to same device as image (GPU)                                      │
│                                                                                                                      │
│ What happens inside tokenization:                                                                                    │
│                                                                                                                      │
│                                                                                                                      │
│  # Example: "No acute findings###" becomes:                                                                          │
│  # Token IDs: [1939, 1274, 1082, 2827, 4136, 4136, 4136]  # Actual numbers vary                                      │
│  # Attention mask: [1, 1, 1, 1, 1, 1, 1]  # 1 = real token, 0 = padding                                              │
│                                                                                                                      │
│                                                                                                                      │
│                                        4. Creating Target Labels for Training                                        │
│                                                                                                                      │
│                                                                                                                      │
│  # Line 220-222 in mini_gpt4.py                                                                                      │
│  targets = to_regress_tokens.input_ids.masked_fill(                                                                  │
│      to_regress_tokens.input_ids == self.llama_tokenizer.pad_token_id, -100                                          │
│  )                                                                                                                   │
│                                                                                                                      │
│                                                                                                                      │
│ Line 220: to_regress_tokens.input_ids - Gets the actual token ID numbers from tokenizer output                       │
│                                                                                                                      │
│ Line 221-222: masked_fill(..., -100) - Replaces padding token IDs with -100                                          │
│                                                                                                                      │
│  • Why -100: PyTorch ignores tokens with label -100 when calculating loss                                            │
│  • Example: [1939, 1274, 0, 0] → [1939, 1274, -100, -100] (where 0 is pad token)                                     │
│                                                                                                                      │
│                                   5. Prompt Text Tokenization (prompt_wrap method)                                   │
│                                                                                                                      │
│                                                                                                                      │
│  # Line 178-181 in mini_gpt4.py                                                                                      │
│  p_before_tokens = self.llama_tokenizer(                                                                             │
│      p_before, return_tensors="pt", add_special_tokens=False).to(img_embeds.device)                                  │
│  p_after_tokens = self.llama_tokenizer(                                                                              │
│      p_after, return_tensors="pt", add_special_tokens=False).to(img_embeds.device)                                   │
│                                                                                                                      │
│                                                                                                                      │
│ Context: This happens when wrapping image with prompt text like "###Patient: <ImageHere> Describe this X-ray         │
│ ###Doctor:"                                                                                                          │
│                                                                                                                      │
│ Line 178-179: Tokenizes text BEFORE image                                                                            │
│                                                                                                                      │
│  • p_before: Text like "###Patient: "                                                                                │
│  • Tokenization: "###Patient: " → [4136, 4136, 4136, 4373, 29901, 29871] (example token IDs)                         │
│                                                                                                                      │
│ Line 180-181: Tokenizes text AFTER image                                                                             │
│                                                                                                                      │
│  • p_after: Text like " Describe this X-ray ###Doctor:"                                                              │
│  • Tokenization: " Describe this X-ray ###Doctor:" → [20355, 445, 1060, 29899, 764, 4136, 4136, 4136, 29928, 20621,  │
│    29901]                                                                                                            │
│                                                                                                                      │
│                                        6. Converting Token IDs to Embeddings                                         │
│                                                                                                                      │
│                                                                                                                      │
│  # Line 182-183 in mini_gpt4.py                                                                                      │
│  p_before_embeds = self.llama_model.model.embed_tokens(p_before_tokens.input_ids).expand(batch_size, -1, -1)         │
│  p_after_embeds = self.llama_model.model.embed_tokens(p_after_tokens.input_ids).expand(batch_size, -1, -1)           │
│                                                                                                                      │
│                                                                                                                      │
│ Line 182: Converting token IDs to embeddings                                                                         │
│                                                                                                                      │
│  • p_before_tokens.input_ids: Token ID tensor like [4136, 4373, 29901]                                               │
│  • self.llama_model.model.embed_tokens(...): This is the embedding lookup!                                           │
│     • Takes token IDs and looks up corresponding embeddings from LLaMA's embedding table                             │
│     • Input: [4136, 4373, 29901] (token IDs)                                                                         │
│     • Output: [[0.1, -0.3, 0.8, ...], [0.5, 0.2, -0.1, ...], [-0.2, 0.9, 0.4, ...]] (embeddings)                     │
│  • .expand(batch_size, -1, -1): Repeats embeddings for each item in batch                                            │
│                                                                                                                      │
│ Line 183: Same process for text after image                                                                          │
│                                                                                                                      │
│                              7. Text Tokenization in Inference (get_context_emb method)                              │
│                                                                                                                      │
│                                                                                                                      │
│  # Line 210-214 in conversation.py                                                                                   │
│  seg_tokens = [                                                                                                      │
│      self.model.llama_tokenizer(                                                                                     │
│          seg, return_tensors="pt", add_special_tokens=i == 0).to(self.device).input_ids                              │
│      for i, seg in enumerate(prompt_segs)                                                                            │
│  ]                                                                                                                   │
│                                                                                                                      │
│                                                                                                                      │
│ Line 211-213: Tokenizes each text segment in conversation                                                            │
│                                                                                                                      │
│  • seg: Text segment like "###Patient: " or " What do you see? ###Doctor:"                                           │
│  • self.model.llama_tokenizer(seg, ...): Converts text segment to token IDs                                          │
│  • add_special_tokens=i == 0: Only adds BOS token to first segment                                                   │
│  • return_tensors="pt": Returns PyTorch tensor                                                                       │
│  • .input_ids: Extracts just the token ID numbers                                                                    │
│                                                                                                                      │
│ Example tokenization:                                                                                                │
│                                                                                                                      │
│                                                                                                                      │
│  # seg = "###Patient: " → seg_tokens = [4136, 4136, 4136, 4373, 29901, 29871]                                        │
│  # seg = " What do you see?" → seg_tokens = [1724, 437, 366, 1074, 29973]                                            │
│                                                                                                                      │
│                                                                                                                      │
│                                   8. Converting Tokens to Embeddings for Inference                                   │
│                                                                                                                      │
│                                                                                                                      │
│  # Line 216 in conversation.py                                                                                       │
│  seg_embs = [self.model.llama_model.model.embed_tokens(seg_t) for seg_t in seg_tokens]                               │
│                                                                                                                      │
│                                                                                                                      │
│ Line 216: Final token-to-embedding conversion                                                                        │
│                                                                                                                      │
│  • seg_t: Token ID tensor for each text segment                                                                      │
│  • self.model.llama_model.model.embed_tokens(seg_t): Looks up embeddings for each token ID                           │
│  • Result: List of embedding tensors, one for each text segment                                                      │
│                                                                                                                      │
│ What happens:                                                                                                        │
│                                                                                                                      │
│                                                                                                                      │
│  # Token IDs: [4136, 4136, 4136, 4373, 29901]                                                                        │
│  # ↓ embed_tokens lookup ↓                                                                                           │
│  # Embeddings: [[0.1, -0.3, 0.8, ...],    # embedding for token 4136                                                 │
│  #              [0.1, -0.3, 0.8, ...],    # embedding for token 4136                                                 │
│  #              [0.1, -0.3, 0.8, ...],    # embedding for token 4136                                                 │
│  #              [0.5, 0.2, -0.1, ...],    # embedding for token 4373                                                 │
│  #              [-0.2, 0.9, 0.4, ...]]    # embedding for token 29901                                                │
│                                                                                                                      │
│                                                                                                                      │
│                                                                                                                      │
│                                           Complete Text Token Flow Summary                                           │
│                                                                                                                      │
│  1 Raw Text: "No acute findings in chest X-ray"                                                                      │
│  2 Text Preprocessing: "No acute findings in chest X-ray###" (add end symbol)                                        │
│  3 Tokenization: [1939, 1274, 1082, 2827, 297, 521, 342, 29899, 764, 4136, 4136, 4136] (token IDs)                   │
│  4 Embedding Lookup: Each token ID → 4096-dimensional embedding vector                                               │
│  5 Final Embeddings: [batch_size, sequence_length, 4096] tensor ready for LLaMA                                      │
│                                                                                                                      │
│ The key insight is that text never directly goes to an "encoder" - instead:                                          │
│                                                                                                                      │
│  • Text → Tokenizer → Token IDs → Embedding Layer → Embeddings → LLaMA Transformer                                   │
│  • The LLaMA model itself acts as both encoder and decoder in the autoregressive generation process                  │
│                                                                                                                      │
│ Would you like me to explain any specific part of this tokenization process in more detail?  
