                          MedCLIP Training Data & Tokenization: Complete Breakdown                             â”ƒ â”‚
â”‚ â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”› â”‚
â”‚                                                                                                                      â”‚
â”‚ Based on my analysis of the MedCLIP codebase, here's exactly what data is used, how it's fed to the model, and the   â”‚
â”‚ tokenization process:                                                                                                â”‚
â”‚                                                                                                                      â”‚
â”‚                                                                                                                      â”‚
â”‚                                                ğŸ“Š Training Data Used                                                 â”‚
â”‚                                                                                                                      â”‚
â”‚                                                  Dataset Structure                                                   â”‚
â”‚                                                                                                                      â”‚
â”‚  â€¢ File: MedCLIP/datav2/captions.csv                                                                                 â”‚
â”‚  â€¢ Total samples: 2,262 image-caption pairs                                                                          â”‚
â”‚  â€¢ Format: image,caption                                                                                             â”‚
â”‚  â€¢ Split: 90% training (~2,036), 10% validation (~226)                                                               â”‚
â”‚                                                                                                                      â”‚
â”‚                                                 Exact Data Examples                                                  â”‚
â”‚                                                                                                                      â”‚
â”‚                                                                                                                      â”‚
â”‚  image,caption                                                                                                       â”‚
â”‚  synpic100052.jpg,"a Multiple or Montage plane MR scan of Spine Trauma on a 57 year old female depicting Traumatic   â”‚
â”‚  Thoracic Aortic Aneurysm"                                                                                           â”‚
â”‚  synpic100082.jpg,"a Multiple or Montage plane MR scan of Spine Trauma on a 52 year old male depicting Disc          â”‚
â”‚  herniation complex at C3-4 and 5-6"                                                                                 â”‚
â”‚  synpic100213.jpg,"a Lateral plane XR scan of Musculoskeletal Trauma on a 24 year old male depicting Fracture of th  â”‚
â”‚  fifth metatarsal"                                                                                                   â”‚
â”‚                                                                                                                      â”‚
â”‚                                                                                                                      â”‚
â”‚                                                                                                                      â”‚
â”‚                                            ğŸ”„ Data Flow Through Training                                             â”‚
â”‚                                                                                                                      â”‚
â”‚                                                 Step 1: Data Loading                                                 â”‚
â”‚                                                                                                                      â”‚
â”‚                                                                                                                      â”‚
â”‚  # From main.ipynb                                                                                                   â”‚
â”‚  def make_train_valid_dfs():                                                                                         â”‚
â”‚      dataframe = pd.read_csv(f"{CFG.captions_path}/captions.csv")                                                    â”‚
â”‚      train, test = train_test_split(dataframe, test_size=.1, train_size=.9,                                          â”‚
â”‚                                     shuffle=True, random_state=77)                                                   â”‚
â”‚      return train, test                                                                                              â”‚
â”‚                                                                                                                      â”‚
â”‚  testing_df, training_df = make_train_valid_dfs()                                                                    â”‚
â”‚                                                                                                                      â”‚
â”‚                                                                                                                      â”‚
â”‚                                               Step 2: Dataset Creation                                               â”‚
â”‚                                                                                                                      â”‚
â”‚                                                                                                                      â”‚
â”‚  class CLIPDataset(torch.utils.data.Dataset):                                                                        â”‚
â”‚      def __init__(self, image_filenames, captions, tokenizer, transforms):                                           â”‚
â”‚          self.image_filenames = image_filenames  # ['synpic100052.jpg', ...]                                         â”‚
â”‚          self.captions = list(captions)          # ['a Multiple or Montage...', ...]                                 â”‚
â”‚                                                                                                                      â”‚
â”‚          # TOKENIZATION HAPPENS HERE - ALL CAPTIONS AT ONCE                                                          â”‚
â”‚          self.encoded_captions = tokenizer(                                                                          â”‚
â”‚              list(captions),           # All captions in dataset                                                     â”‚
â”‚              padding=True,             # Pad to same length                                                          â”‚
â”‚              truncation=True,          # Cut if too long                                                             â”‚
â”‚              max_length=CFG.max_length # 200 tokens                                                                  â”‚
â”‚          )                                                                                                           â”‚
â”‚                                                                                                                      â”‚
â”‚                                                                                                                      â”‚
â”‚                                             Step 3: Tokenization Process                                             â”‚
â”‚                                                                                                                      â”‚
â”‚ Tokenizer Used: Bio_ClinicalBERT (medical domain BERT)                                                               â”‚
â”‚                                                                                                                      â”‚
â”‚ Example Tokenization:                                                                                                â”‚
â”‚                                                                                                                      â”‚
â”‚                                                                                                                      â”‚
â”‚  # Input text:                                                                                                       â”‚
â”‚  caption = "a Multiple or Montage plane MR scan of Spine Trauma on a 57 year old female depicting Traumatic Thoraci  â”‚
â”‚  Aortic Aneurysm"                                                                                                    â”‚
â”‚                                                                                                                      â”‚
â”‚  # After tokenization:                                                                                               â”‚
â”‚  encoded = tokenizer(caption, padding=True, truncation=True, max_length=200)                                         â”‚
â”‚                                                                                                                      â”‚
â”‚  # Result:                                                                                                           â”‚
â”‚  {                                                                                                                   â”‚
â”‚      'input_ids': [101, 1037, 3674, 2030, 15805, 4946, 2720, 26061, 1997, 8560, 11105, 2006, 1037, 4293, 2095, 2214  â”‚
â”‚  2931, 11230, 11105, 11105, 102, 0, 0, ...],                                                                         â”‚
â”‚      'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, ...]                    â”‚
â”‚  }                                                                                                                   â”‚
â”‚                                                                                                                      â”‚
â”‚                                                                                                                      â”‚
â”‚ Token Breakdown:                                                                                                     â”‚
â”‚                                                                                                                      â”‚
â”‚  â€¢ 101 = [CLS] (start token)                                                                                         â”‚
â”‚  â€¢ 1037 = "a"                                                                                                        â”‚
â”‚  â€¢ 3674 = "multiple"                                                                                                 â”‚
â”‚  â€¢ 2030 = "or"                                                                                                       â”‚
â”‚  â€¢ 15805 = "montage"                                                                                                 â”‚
â”‚  â€¢ 4946 = "plane"                                                                                                    â”‚
â”‚  â€¢ 2720 = "mr"                                                                                                       â”‚
â”‚  â€¢ 26061 = "scan"                                                                                                    â”‚
â”‚  â€¢ 102 = [SEP] (end token)                                                                                           â”‚
â”‚  â€¢ 0 = [PAD] (padding)                                                                                               â”‚
â”‚                                                                                                                      â”‚
â”‚                                                Step 4: Batch Creation                                                â”‚
â”‚                                                                                                                      â”‚
â”‚                                                                                                                      â”‚
â”‚  def build_loaders(dataframe, tokenizer, mode):                                                                      â”‚
â”‚      dataset = CLIPDataset(                                                                                          â”‚
â”‚          dataframe["image"].values,    # Image filenames                                                             â”‚
â”‚          dataframe["caption"].values,  # Medical captions                                                            â”‚
â”‚          tokenizer=tokenizer,          # Bio_ClinicalBERT                                                            â”‚
â”‚          transforms=transforms         # Image preprocessing                                                         â”‚
â”‚      )                                                                                                               â”‚
â”‚                                                                                                                      â”‚
â”‚      dataloader = torch.utils.data.DataLoader(                                                                       â”‚
â”‚          dataset,                                                                                                    â”‚
â”‚          batch_size=12,               # 12 samples per batch                                                         â”‚
â”‚          num_workers=2,                                                                                              â”‚
â”‚          shuffle=True                                                                                                â”‚
â”‚      )                                                                                                               â”‚
â”‚                                                                                                                      â”‚
â”‚                                                                                                                      â”‚
â”‚                                            Step 5: Single Item Processing                                            â”‚
â”‚                                                                                                                      â”‚
â”‚                                                                                                                      â”‚
â”‚  def __getitem__(self, idx):                                                                                         â”‚
â”‚      # Get pre-tokenized text                                                                                        â”‚
â”‚      item = {                                                                                                        â”‚
â”‚          'input_ids': torch.tensor(self.encoded_captions['input_ids'][idx]),                                         â”‚
â”‚          'attention_mask': torch.tensor(self.encoded_captions['attention_mask'][idx])                                â”‚
â”‚      }                                                                                                               â”‚
â”‚                                                                                                                      â”‚
â”‚      # Load and process image                                                                                        â”‚
â”‚      image = cv2.imread(f"{CFG.image_path}/{self.image_filenames[idx]}")                                             â”‚
â”‚      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)                                                                  â”‚
â”‚      image = self.transforms(image=image)['image']  # Resize to 224x224, normalize                                   â”‚
â”‚      item['image'] = torch.tensor(image).permute(2, 0, 1).float()                                                    â”‚
â”‚      item['caption'] = self.captions[idx]  # Original text (for reference)                                           â”‚
â”‚                                                                                                                      â”‚
â”‚      return item                                                                                                     â”‚
â”‚                                                                                                                      â”‚
â”‚                                                                                                                      â”‚
â”‚                                                                                                                      â”‚
â”‚                                             ğŸ¯ Training Batch Structure                                              â”‚
â”‚                                                                                                                      â”‚
â”‚ Each training batch contains:                                                                                        â”‚
â”‚                                                                                                                      â”‚
â”‚                                                                                                                      â”‚
â”‚  batch = {                                                                                                           â”‚
â”‚      'image': torch.tensor([12, 3, 224, 224]),      # 12 medical images                                              â”‚
â”‚      'input_ids': torch.tensor([12, 200]),          # 12 tokenized captions                                          â”‚
â”‚      'attention_mask': torch.tensor([12, 200]),     # 12 attention masks                                             â”‚
â”‚      'caption': ['caption1', 'caption2', ...]       # 12 original texts (not used in forward pass)                   â”‚
â”‚  }                                                                                                                   â”‚
â”‚                                                                                                                      â”‚
â”‚                                                                                                                      â”‚
â”‚                                                                                                                      â”‚
â”‚                                                ğŸ”¥ Model Forward Pass                                                 â”‚
â”‚                                                                                                                      â”‚
â”‚                                                                                                                      â”‚
â”‚  def forward(self, batch):                                                                                           â”‚
â”‚      # Encode images: (12, 3, 224, 224) -> (12, 2048)                                                                â”‚
â”‚      image_features = self.image_encoder(batch["image"])                                                             â”‚
â”‚                                                                                                                      â”‚
â”‚      # Encode text: (12, 200) -> (12, 768)                                                                           â”‚
â”‚      text_features = self.text_encoder(                                                                              â”‚
â”‚          input_ids=batch["input_ids"],                                                                               â”‚
â”‚          attention_mask=batch["attention_mask"]                                                                      â”‚
â”‚      )                                                                                                               â”‚
â”‚                                                                                                                      â”‚
â”‚      # Project to same dimension: (12, 256)                                                                          â”‚
â”‚      image_embeddings = self.image_projection(image_features)                                                        â”‚
â”‚      text_embeddings = self.text_projection(text_features)                                                           â”‚
â”‚                                                                                                                      â”‚
â”‚      # Calculate contrastive loss                                                                                    â”‚
â”‚      logits = (text_embeddings @ image_embeddings.T) / self.temperature                                              â”‚
â”‚      # ... loss calculation ...                                                                                      â”‚
â”‚                                                                                                                      â”‚
â”‚                                                                                                                      â”‚
â”‚                                                                                                                      â”‚
â”‚                                                ğŸ“ Text Structure Used                                                â”‚
â”‚                                                                                                                      â”‚
â”‚ Template: "a [Plane] plane [Modality] scan of [Location] [Category] on a [Age] year old [Gender] depicting           â”‚
â”‚ [Diagnosis]"                                                                                                         â”‚
â”‚                                                                                                                      â”‚
â”‚ Components:                                                                                                          â”‚
â”‚                                                                                                                      â”‚
â”‚  â€¢ Plane: Axial, Sagittal, Coronal, Multiple or Montage                                                              â”‚
â”‚  â€¢ Modality: MR, CT, XR (X-Ray)                                                                                      â”‚
â”‚  â€¢ Location: Spine, Brain and Neuro, Chest, Musculoskeletal                                                          â”‚
â”‚  â€¢ Category: Trauma, Neoplasm, Infection, etc.                                                                       â”‚
â”‚  â€¢ Demographics: Age + Gender                                                                                        â”‚
â”‚  â€¢ Diagnosis: Detailed medical condition                                                                             â”‚
â”‚                                                                                                                      â”‚
â”‚                                                                                                                      â”‚
â”‚                                                ğŸ¯ Key Training Facts                                                 â”‚
â”‚                                                                                                                      â”‚
â”‚  â€¢ Total training samples: ~2,036 medical image-caption pairs                                                        â”‚
â”‚  â€¢ Batch size: 12 pairs per batch                                                                                    â”‚
â”‚  â€¢ Text tokenizer: Bio_ClinicalBERT (medical domain)                                                                 â”‚
â”‚  â€¢ Max text length: 200 tokens                                                                                       â”‚
â”‚  â€¢ Image processing: Resize to 224x224, normalize                                                                    â”‚
â”‚  â€¢ Training epochs: 2                                                                                                â”‚
â”‚  â€¢ Learning objective: Contrastive alignment between medical images and clinical text                                â”‚
â”‚                                                                                                                      â”‚
â”‚ The model learns to create a shared embedding space where medical images and their corresponding diagnostic          â”‚
â”‚ descriptions have high similarity, enabling zero-shot medical image captioning and text-based medical image          â”‚
â”‚ retrieval.                                                                                                      
